import os
import math
import random
import shutil
os.environ['TF_CPP_MIN_LOG_LEVLE'] = '2'

import scipy
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn
import time
from tensorflow.keras.metrics import AUC

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

names = ["benign", "malignant"]

# Load the base EfficientNetB4 model (without the final classification layers)
b4_model = tf.keras.applications.EfficientNetB4(
    include_top=False,  # Exclude the final classification layer
    weights='imagenet',  # Use pre-trained ImageNet weights
    input_shape=(380, 380, 3)  # Input shape of the images
)

b4_model.summary()

b4_model.trainable = False  # Freeze the base model's weights

# Define the custom model
inputs = keras.Input(shape=(380, 380, 3))
x = b4_model(inputs, training=False)  # Pass inputs into the base model
x = layers.GlobalAveragePooling2D()(x)  # Global average pooling layer
x = layers.Dense(2)(x)  # Final dense layer with 2 output classes (logits)

model = keras.Model(inputs, x)  # Define the complete model

# Summary of the new model
model.summary()


# loss and optimizer
loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)
optim = keras.optimizers.Adam(learning_rate=0.001)
metrics = ["accuracy"]

model.compile(optimizer=optim, loss=loss, metrics=metrics)

# Correct reference to the preprocess_input function for EfficientNetB4

preprocess_input = tf.keras.applications.efficientnet.preprocess_input

# Generate batches of tensor image data with real-time dataaugmentation
train_gen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)
          #rotation_range=20,
          #horizontal_flip=True,
          #width_shift_range=0.2,
          #height_shift_range=0.2,
          #shear_range=0.2,
          #zoom_range=0.2)
test_gen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)
valid_gen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)

train_batches = train_gen.flow_from_directory(
  "B:\\Cancer\\BreaKHis 400X\\BreaKHis 400X\\train",
  target_size=(380,380),
  class_mode='sparse',
  batch_size=32,
  shuffle=True,
  color_mode="rgb",
  classes=names
)

test_batches = train_gen.flow_from_directory(
  "B:\\Cancer\\BreaKHis 400X\\BreaKHis 400X\\test",
  target_size=(380,380),
  class_mode='sparse',
  batch_size=32,
  shuffle=False,
  color_mode="rgb",
  classes=names
)

valid_batches = train_gen.flow_from_directory(
  "B:\\Cancer\\BreaKHis 400X\\BreaKHis 400X\\valid",
  target_size=(380,380),
  class_mode='sparse',
  batch_size=32,
  shuffle=False,
  color_mode="rgb",
  classes=names
)

train_batch = train_batches[0]
print(train_batch[0].shape)
print(train_batch[1])
test_batch = test_batches[0]
print(test_batch[0].shape)
print(test_batch[1])

# check the image after augmentation
def show(batch, pred_labels=None):
  plt.figure(figsize=(10,10))
  for i in range(4):
    plt.subplot(2,2,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(batch[0][i], cmap=plt.cm.binary)
    lbl = names[int(batch[1][i])]
    if pred_labels is not None:
      lbl += "/Pred: " + names[int(pred_labels[i])]
    plt.xlabel(lbl)
  plt.show()

show(test_batch)
show(train_batch)

#training
epochs = 30

# callback
early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=2)
log_BC_EFFB4 = model.fit(train_batches, validation_data=valid_batches,
                callbacks=[early_stopping],
                epochs=epochs, verbose=2)

model.save("TL_BC_EFFB4.keras")

# plot loss and acc
plt.figure(figsize=(16,6))
plt.subplot(1,2,1)
plt.plot(log_BC_EFFB4.history['loss'], label='train loss')
plt.plot(log_BC_EFFB4.history['val_loss'], label='valid loss')
plt.grid()
plt.legend(fontsize=15)
plt.subplot(1,2,2)
plt.plot(log_BC_EFFB4.history['accuracy'], label='train acc')
plt.plot(log_BC_EFFB4.history['val_accuracy'], label='valid acc')
plt.grid()
plt.legend(fontsize=15)

model.evaluate(test_batches, verbose=2)

# Get the predictions from the model
start_time = time.time() # inference time
predictions = model.predict(test_batches, verbose=2)  # Predict on the test batches
predictions = tf.nn.softmax(predictions).numpy()  # Apply softmax to get probabilities
end_time = time.time()
print(f"Inference Time: {end_time - start_time:.2f} seconds")

# Get the predicted class for each image
predicted_labels = np.argmax(predictions, axis=1)  # Index of the maximum probability gives the predicted class

# Get the true labels from the test_batches (ground truth)
true_labels = test_batches.classes  # Ground truth labels from the ImageDataGenerator

# Class names (e.g., "Bird-drop", "Clean", etc.)
class_indices = list(test_batches.class_indices.keys())

# Show some sample predictions with their true labels
print("Sample Inference Results:")
for i in range(10):  # Display 10 sample results
    print(f"Image {i + 1}:")
    print(f"  Predicted: {class_indices[predicted_labels[i]]} (Class {predicted_labels[i]})")
    print(f"  True: {class_indices[true_labels[i]]} (Class {true_labels[i]})\n")

# Optionally, plot the first few test images and annotate them with their predictions
plt.figure(figsize=(15, 10))
# 
for batch_idx, (batch_images, batch_labels) in enumerate(test_batches):  # 循环处理测试批次
    if batch_idx >= 9:  # show 9 PNG
        break
    plt.subplot(3, 3, batch_idx + 1)
    img, lbl = batch_images[0], batch_labels[0]  # 取第一个图片和标签
    plt.imshow(img)
    plt.title(f"Predicted: {class_indices[predicted_labels[batch_idx]]}\nTrue: {class_indices[int(lbl)]}")
    plt.axis("off")

plt.tight_layout()
plt.show()

def calculate_auc(true_labels, predictions):
  auc_metric = AUC()
  auc_metric.update_state(true_labels, predictions)
  return auc_metric.result().numpy()


# Manual F1-score calculation
def calculate_f1_score(true_labels, predicted_labels):
  true_positive = np.sum((true_labels == predicted_labels) & (true_labels == 1))
  false_positive = np.sum((true_labels != predicted_labels) & (predicted_labels == 1))
  false_negative = np.sum((true_labels != predicted_labels) & (true_labels == 1))
  precision = true_positive / (true_positive + false_positive + 1e-7)
  recall = true_positive / (true_positive + false_negative + 1e-7)
  f1 = 2 * (precision * recall) / (precision + recall + 1e-7)
  return f1


try:
  auc_score = calculate_auc(true_labels, predictions[:, 1])  # Use the second column for positive predictions
  print(f"AUC: {auc_score:.4f}")
except ValueError as e:
  print(f"AUC Calculation Error: {e}")

f1 = calculate_f1_score(true_labels, predicted_labels)
print(f"F1-score: {f1:.4f}")
