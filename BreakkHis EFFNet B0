import os
import math
import random
import shutil
os.environ['TF_CPP_MIN_LOG_LEVLE'] = '2'

import scipy
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn
import time
from tensorflow.keras.metrics import AUC

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

names = ["benign", "malignant"]

# Load the base EfficientNetB0 model (without the final classification layers)
b0_model = tf.keras.applications.EfficientNetB0(
    include_top=False,  # Exclude the final classification layer
    weights='imagenet',  # Use pre-trained ImageNet weights
    input_shape=(224, 224, 3)  # Input shape of the images
)

b0_model.summary()

b0_model.trainable = False  # Freeze the base model's weights

# Define the custom model
inputs = keras.Input(shape=(224, 224, 3))
x = b0_model(inputs, training=False)  # Pass inputs into the base model
x = layers.GlobalAveragePooling2D()(x)  # Global average pooling layer
x = layers.Dense(2, activation='softmax')(x)  # Final dense layer with 6 output classes (logits)

model = keras.Model(inputs, x)  # Define the complete model

# Summary of the new model
model.summary()


# loss and optimizer
#loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)
optim = keras.optimizers.Adam(learning_rate=0.001)
metrics = ["accuracy"]

model.compile(optimizer=optim, loss="categorical_crossentropy", metrics=metrics)

# Correct reference to the preprocess_input function for EfficientNetB4

preprocess_input = tf.keras.applications.efficientnet.preprocess_input

# Generate batches of tensor image data with real-time dataaugmentation
train_gen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)
          #rotation_range=20,
          #horizontal_flip=True,
          #width_shift_range=0.2,
          #height_shift_range=0.2,
          #shear_range=0.2,
          #zoom_range=0.2)
test_gen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)
valid_gen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)

train_batches = train_gen.flow_from_directory(
  "B:\\Cancer\\BreaKHis 400X\\BreaKHis 400X\\train",
  target_size=(224, 224),
  class_mode='categorical',
  batch_size=32,
  shuffle=True,
  color_mode="rgb",
  classes=names
)

test_batches = train_gen.flow_from_directory(
  "B:\\Cancer\\BreaKHis 400X\\BreaKHis 400X\\test",
  target_size=(224, 224),
  class_mode='categorical',
  batch_size=32,
  shuffle=False,
  color_mode="rgb",
  classes=names
)

valid_batches = train_gen.flow_from_directory(
  "B:\\Cancer\\BreaKHis 400X\\BreaKHis 400X\\valid",
  target_size=(224, 224),
  class_mode='categorical',
  batch_size=32,
  shuffle=False,
  color_mode="rgb",
  classes=names
)

train_batch = train_batches[0]
print(train_batch[0].shape)
print(train_batch[1])
test_batch = test_batches[0]
print(test_batch[0].shape)
print(test_batch[1])

# check the image after augmentation
def show(batch, pred_labels=None):
  plt.figure(figsize=(10,10))
  for i in range(4):
    plt.subplot(2,2,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(batch[0][i], cmap=plt.cm.binary)
    lbl = names[np.argmax(batch[1][i])]
    if pred_labels is not None:
      lbl += "/Pred: " + names[int(pred_labels[i])]
    plt.xlabel(lbl)
  plt.show()

show(test_batch)
show(train_batch)

#training
epochs = 30

# callback
early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=2)
log_BC_EFFB0 = model.fit(train_batches, validation_data=valid_batches,
                callbacks=[early_stopping],
                epochs=epochs, verbose=2)

model.save("TL_BC_EFFB4.keras")

# plot loss and acc
plt.figure(figsize=(16,6))
plt.subplot(1,2,1)
plt.plot(log_BC_EFFB0.history['loss'], label='train loss')
plt.plot(log_BC_EFFB0.history['val_loss'], label='valid loss')
plt.grid()
plt.legend(fontsize=15)
plt.subplot(1,2,2)
plt.plot(log_BC_EFFB0.history['accuracy'], label='train acc')
plt.plot(log_BC_EFFB0.history['val_accuracy'], label='valid acc')
plt.grid()
plt.legend(fontsize=15)

model.evaluate(test_batches, verbose=2)

# Get the predictions from the model
start_time = time.time() # inference time
predictions = model.predict(test_batches, verbose=2)  # Predict on the test batches
predictions = tf.nn.softmax(predictions).numpy()  # Apply softmax to get probabilities
end_time = time.time()
print(f"Inference Time: {end_time - start_time:.2f} seconds")

# Get the predicted class for each image
predicted_labels = np.argmax(predictions, axis=1)  # Index of the maximum probability gives the predicted class

# Get the true labels from the test_batches (ground truth)
true_labels = test_batches.classes  # Ground truth labels from the ImageDataGenerator

# Class names (e.g., "Bird-drop", "Clean", etc.)
class_indices = list(test_batches.class_indices.keys())

# Show some sample predictions with their true labels
print("Sample Inference Results:")
for i in range(10):  # Display 10 sample results
    print(f"Image {i + 1}:")
    print(f"  Predicted: {class_indices[predicted_labels[i]]} (Class {predicted_labels[i]})")
    print(f"  True: {class_indices[true_labels[i]]} (Class {true_labels[i]})\n")

# Optionally, plot the first few test images and annotate them with their predictions
plt.figure(figsize=(15, 10))
# 正确的方式是遍历批次数据，确保索引不会超出范围
for batch_idx, (batch_images, batch_labels) in enumerate(test_batches):  # 循环处理测试批次
    if batch_idx >= 9:  # 仅绘制前9张图片
        break
    plt.subplot(3, 3, batch_idx + 1)
    img, lbl = batch_images[0], batch_labels[0]  # 取第一个图片和标签
    plt.imshow(img)
    plt.title(f"Predicted: {class_indices[predicted_labels[batch_idx]]}\nTrue: {class_indices[np.argmax(lbl)]}")
    plt.axis("off")

plt.tight_layout()
plt.show()

# Calculate confusion matrix
def calculate_confusion_matrix(true_labels, predicted_labels):
    # 计算 TP, FP, TN, FN
    TP = np.sum((predicted_labels == 1) & (true_labels == 1))
    FP = np.sum((predicted_labels == 1) & (true_labels == 0))
    TN = np.sum((predicted_labels == 0) & (true_labels == 0))
    FN = np.sum((predicted_labels == 0) & (true_labels == 1))

    return TP, FP, TN, FN


def calculate_sensitivity_specificity(TP, FP, TN, FN):
    sensitivity = TP / (TP + FN + 1e-7)
    specificity = TN / (TN + FP + 1e-7)
    return sensitivity, specificity

# Calculate AUC
def calculate_auc(true_labels, predictions):
  auc_metric = AUC()
  auc_metric.update_state(true_labels, predictions)
  return auc_metric.result().numpy()


# Manual F1-score calculation
def calculate_f1_score(true_labels, predicted_labels):
  true_positive = np.sum((true_labels == predicted_labels) & (true_labels == 1))
  false_positive = np.sum((true_labels != predicted_labels) & (predicted_labels == 1))
  false_negative = np.sum((true_labels != predicted_labels) & (true_labels == 1))
  precision = true_positive / (true_positive + false_positive + 1e-7)
  recall = true_positive / (true_positive + false_negative + 1e-7)
  f1 = 2 * (precision * recall) / (precision + recall + 1e-7)
  return f1


try:
  auc_score = calculate_auc(true_labels, predictions[:, 1])  # Directly use predictions as they represent positive class probabilities
  print(f"AUC: {auc_score:.4f}")
except ValueError as e:
  print(f"AUC Calculation Error: {e}")

# Convert one-hot encoded ground-truth to indices
true_labels = test_batches.classes
f1 = calculate_f1_score(true_labels, predicted_labels)
print(f"F1-score: {f1:.4f}")

# print confusion matrix
TP, FP, TN, FN = calculate_confusion_matrix(true_labels, predicted_labels)
print("Confusion Matrix:")
print(f"{'':<10} {'Predicted 1':<12} {'Predicted 0':<12}")
print(f"{'Actual 1':<10} {TP:<12} {FN:<12}")
print(f"{'Actual 0':<10} {FP:<12} {TN:<12}")

#  Calculate sensitivity and specificity
sensitivity, specificity = calculate_sensitivity_specificity(TP, FP, TN, FN)
print(f"\nSensitivity (Recall): {sensitivity:.4f}")
print(f"Specificity: {specificity:.4f}")
